{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0cc201",
   "metadata": {},
   "source": [
    "# Finetuning the original model\n",
    "This notebook simply does the initial finetuning of the model - 5 epochs, default LLM learning rate, etc.\n",
    "The weights of the result will be saved and used in a separate notebook looking at the actual distillation.\n",
    "Common functions/classes will pobably be put in a separate script once this is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74da4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LayoutLMv3Processor,\n",
    "    LayoutLMv3ForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9670b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create runtime vars\n",
    "categories = [\"address\", \"company\", \"date\", \"total\", \"none\"]\n",
    "cat_id_map = dict(enumerate(categories))\n",
    "id_cat_map = {v:k for k,v in cat_id_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53ace8",
   "metadata": {},
   "source": [
    "### Define dataset & preprocessing funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8d46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_SROIE_csv(filepath: str, delim: str = \",\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Write a quick and dirty method to deal with the fact that the text in the csv file has commas \n",
    "    and so is not interpreted as fixed-width by the default pandas read_csv method.\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        file = f.readlines()\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    p3 = []\n",
    "    p4 = []\n",
    "    text = []\n",
    "    for line in file:\n",
    "        split_data = line.split(delim)\n",
    "        # get bbox coords\n",
    "        p1.append(split_data[:2])\n",
    "        p2.append(split_data[2:4])\n",
    "        p3.append(split_data[4:6])\n",
    "        p4.append(split_data[6:8])\n",
    "        # get text data\n",
    "        text.append(\" \".join(split_data[8:]).replace(\"\\n\", \"\"))\n",
    "    df = pd.DataFrame({\n",
    "        \"p1\": p1,\n",
    "        \"p2\": p2,\n",
    "        \"p3\": p3,\n",
    "        \"p4\": p4,\n",
    "        \"text\": text\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f271cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_bbox_to_word_level(df: pd.DataFrame) -> Tuple[List[str], List[List[int]]]:\n",
    "    \"\"\"\n",
    "    The bounding box data is not on the word level, and is instead on a string-of-words level.\n",
    "    This function splits the data down to a word level by letting multiple words share a single\n",
    "    bounding box.\n",
    "    \"\"\"\n",
    "    if \"bbox\" not in df.columns or \"text\" not in df.columns:\n",
    "        raise ValueError(\"df does not have bbox or text column!\")\n",
    "    words = []\n",
    "    bboxes = []\n",
    "    for row in df.itertuples():\n",
    "        text = row.text\n",
    "        bbox = row.bbox\n",
    "        text_split = text.split(\" \")\n",
    "        dup_bbox = [bbox] * len(text_split)\n",
    "        words.extend(text_split)\n",
    "        bboxes.extend(dup_bbox)\n",
    "    return words, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44434b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten key dictionary to a word level, with each word getting the label it appears under\n",
    "def flatten_keys(keys: dict) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Brings the labels to the word level\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    labels = []\n",
    "    for label, sequence in keys.items():\n",
    "        seq = sequence.replace(\",\", \" \")\n",
    "        word_split = [word for word in seq.split(\" \") if word != \"\"]\n",
    "        words.extend(word_split)\n",
    "        labels.extend([label] * len(word_split))\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf86e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SROIEProcDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Takes care of the processing of each document for use by the model\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        self.processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "        # read in data on disk - sort to ensure data is accessed in the same order\n",
    "        img = sorted(os.listdir(f\"{filepath}/data/img\"))\n",
    "        bbox = sorted(os.listdir(f\"{filepath}/data/box\"))\n",
    "        key = sorted(os.listdir(f\"{filepath}/data/key\"))\n",
    "        # check the dataset is valid\n",
    "        if len(img) != len(bbox) or len(bbox) != len(key):\n",
    "            raise RuntimeError(\"Different number of documents with images and bounding box data\")\n",
    "        # preprocess filepaths so no extra processing needs to be done\n",
    "        self.img = [f\"{filepath}/data/img/{f}\" for f in img]\n",
    "        self.bbox = [f\"{filepath}/data/box/{f}\" for f in bbox]\n",
    "        self.keys = [f\"{filepath}/data/key/{f}\" for f in key]\n",
    "        \n",
    "    def __process_bbox(self, bbox):\n",
    "        \"\"\"\n",
    "        Processes each bounding box in the data into a LLMv3-compatible format\n",
    "        Could probably be a static method, but not important right now\n",
    "        \"\"\"\n",
    "        proc_box = bbox.apply(lambda x: x[0] + x[2], axis = 1).tolist()\n",
    "        bbox[\"bbox\"] = proc_box\n",
    "        bbox = bbox.drop([\"p1\", \"p2\", \"p3\", \"p4\"], axis=1)\n",
    "        # split the bbox data so we predict on the word level - train on this\n",
    "        # we will combine this data back to the bbox level after predictions occur\n",
    "        word, bbox = split_bbox_to_word_level(bbox)\n",
    "        bboxes = pd.DataFrame({\"word\": word, \"bbox\": bbox})\n",
    "        # remove rows that are empty\n",
    "        bboxes = bboxes[bboxes.word != \"\"]\n",
    "        # normalise the bboxes to be in [0, 1000]\n",
    "        bboxes_norm = bboxes.apply(\n",
    "            lambda row: [\n",
    "                int(row[1][0])/image.width * 1000, \n",
    "                int(row[1][1])/image.height * 1000,\n",
    "                int(row[1][2])/image.width * 1000, \n",
    "                int(row[1][3])/image.height * 1000\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        bboxes[\"bbox\"] = bboxes_norm\n",
    "        return bboxes\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img[idx]).convert(\"RGB\")\n",
    "        bbox = read_SROIE_csv(self.bbox[idx])\n",
    "        # process bboxes into LLMv3 format\n",
    "        bboxes = self.__process_bbox(bbox)        \n",
    "        # retrieve keys - these are the labels that we want to process on\n",
    "        with open(self.keys[idx]) as f:\n",
    "            keys = json.load(f)\n",
    "        words, labels = flatten_keys(keys)\n",
    "        labels = pd.DataFrame({\"word\": words, \"label\": labels})\n",
    "        # now match the label to the word-bbox pairs\n",
    "        bbox_labels = []\n",
    "        for word in bboxes.word:\n",
    "            item = labels[labels.word == word].label.tolist()\n",
    "            item = [\"none\"] if item == [] else item\n",
    "            bbox_labels.extend(item)\n",
    "        bboxes[\"label\"] = bbox_labels\n",
    "        # now format as a dict to be fed into the processor\n",
    "        words = bboxes.word.tolist()\n",
    "        boxes = bboxes.bbox.tolist()\n",
    "        labels = bboxes.label.tolist()\n",
    "        encoding = self.processor(\n",
    "            image, \n",
    "            words, \n",
    "            boxes=boxes, \n",
    "            word_labels=labels, \n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        return encoding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "704b70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = str(Path(\"ICDAR-2019-SROIE\").absolute())\n",
    "dataset = SROIEProcDataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405561d",
   "metadata": {},
   "source": [
    "### Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb70a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
