{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a437c32d",
   "metadata": {},
   "source": [
    "# Where I'm up to\n",
    "* Overall aim: Distill LLMv3 on SROIE\n",
    "* Steps needed still:\n",
    "    * Finetune LLMv3 (NER) on SROIE\n",
    "        * Get dataset working\n",
    "            * Decide on method to extract text - some simple heuristics, needs some data exploration to get nice\n",
    "                * Current method of abusing substrings/using Levenshtein distance almost works nicely - need to check which type of values have been/can be split over multiple lines!\n",
    "            * Instead of above approach, split to word level and then make classifications! *Should* make assigning labels programatically easier\n",
    "    * Decide on distillation method (maybe write a function for each of these?)\n",
    "    * Distill model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "38c5bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LayoutLMv3Processor,\n",
    "    LayoutLMv3ForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "52087c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create runtime vars\n",
    "categories = [\"address\", \"company\", \"date\", \"total\", \"none\"]\n",
    "cat_id_map = dict(enumerate(categories))\n",
    "id_cat_map = {v:k for k,v in cat_id_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5bf71",
   "metadata": {},
   "source": [
    "### Define dataset & preprocessing funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3e2fe1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_SROIE_csv(filepath: str, delim: str = \",\") -> pd.DataFrame:\n",
    "    with open(filepath) as f:\n",
    "        file = f.readlines()\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    p3 = []\n",
    "    p4 = []\n",
    "    text = []\n",
    "    for line in file:\n",
    "        split_data = line.split(delim)\n",
    "        # get bbox coords\n",
    "        p1.append(split_data[:2])\n",
    "        p2.append(split_data[2:4])\n",
    "        p3.append(split_data[4:6])\n",
    "        p4.append(split_data[6:8])\n",
    "        # get text data\n",
    "        text.append(\" \".join(split_data[8:]).replace(\"\\n\", \"\"))\n",
    "    df = pd.DataFrame({\n",
    "        \"p1\": p1,\n",
    "        \"p2\": p2,\n",
    "        \"p3\": p3,\n",
    "        \"p4\": p4,\n",
    "        \"text\": text\n",
    "    })\n",
    "    # df.set_index(\"text\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "561fbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(text: str, options: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    For each option in option, returns one of:\n",
    "     * If the option contains the supplied text verbatim, 0\n",
    "    * If the option contains the supplied text, the length-normalised Levenshtein (edit) distance\n",
    "    \"\"\"\n",
    "    sim = []\n",
    "    for option in options:\n",
    "        if text in option or option in text:  # potentially capturing too many matches with the second one\n",
    "            sim.append(0)\n",
    "        else:\n",
    "            sim.append(distance(text, option))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa5eb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_bbox_to_word_level(df: pd.DataFrame) -> Tuple[List[str], List[List[int]]]:\n",
    "    if \"bbox\" not in df.columns or \"text\" not in df.columns:\n",
    "        raise ValueError(\"df does not have bbox or text column!\")\n",
    "    words = []\n",
    "    bboxes = []\n",
    "    for row in df.itertuples():\n",
    "        text = row.text\n",
    "        bbox = row.bbox\n",
    "        text_split = text.split(\" \")\n",
    "        dup_bbox = [bbox] * len(text_split)\n",
    "        words.extend(text_split)\n",
    "        bboxes.extend(dup_bbox)\n",
    "    return words, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5dd58d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten key dictionary to a word level, with each word getting the label it appears under\n",
    "def flatten_keys(keys: dict) -> Tuple[List[str], List[str]]:\n",
    "    words = []\n",
    "    labels = []\n",
    "    for label, sequence in keys.items():\n",
    "        seq = sequence.replace(\",\", \" \")\n",
    "        word_split = [word for word in seq.split(\" \") if word != \"\"]\n",
    "        words.extend(word_split)\n",
    "        labels.extend([label] * len(word_split))\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0732b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SROIEProcDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        self.processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "        # read in data on disk - sort to ensure data is accessed in the same order\n",
    "        img = sorted(os.listdir(f\"{filepath}/data/img\"))\n",
    "        bbox = sorted(os.listdir(f\"{filepath}/data/box\"))\n",
    "        key = sorted(os.listdir(f\"{filepath}/data/key\"))\n",
    "        # check the dataset is valid\n",
    "        if len(img) != len(bbox) or len(bbox) != len(key):\n",
    "            raise RuntimeError(\"Different number of documents with images and bounding box data\")\n",
    "        # preprocess filepaths so no extra processing needs to be done\n",
    "        self.img = [f\"{filepath}/data/img/{f}\" for f in img]\n",
    "        self.bbox = [f\"{filepath}/data/box/{f}\" for f in bbox]\n",
    "        self.keys = [f\"{filepath}/data/key/{f}\" for f in key]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img[idx]).convert(\"RGB\")\n",
    "        bbox = read_SROIE_csv(self.bbox[idx])\n",
    "        # process bboxes into LLMv3 format\n",
    "        proc_box = bbox.apply(lambda x: x[0] + x[2], axis = 1).tolist()\n",
    "        bbox[\"bbox\"] = proc_box\n",
    "        bbox = bbox.drop([\"p1\", \"p2\", \"p3\", \"p4\"], axis=1)\n",
    "        # split the bbox data so we predict on the word level - train on this\n",
    "        # we will combine this data back to the bbox level after predictions occur\n",
    "        word, bbox = split_bbox_to_word_level(bbox)\n",
    "        bboxes = pd.DataFrame({\"word\": word, \"bbox\": bbox})\n",
    "        # remove rows that are empty\n",
    "        bboxes = bboxes[bboxes.word != \"\"]\n",
    "        # normalise the bboxes to be in [0, 1000]\n",
    "        bboxes_norm = bboxes.apply(\n",
    "            lambda row: [\n",
    "                int(row[1][0])/image.width * 1000, \n",
    "                int(row[1][1])/image.height * 1000,\n",
    "                int(row[1][2])/image.width * 1000, \n",
    "                int(row[1][3])/image.height * 1000\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        bboxes[\"bbox\"] = bboxes_norm\n",
    "        # retrieve keys - these are the labels that we want to process on\n",
    "        with open(self.keys[idx]) as f:\n",
    "            keys = json.load(f)\n",
    "        words, labels = flatten_keys(keys)\n",
    "        labels = pd.DataFrame({\"word\": words, \"label\": labels})\n",
    "        # now match the label to the word-bbox pairs\n",
    "        bbox_labels = []\n",
    "        for word in bboxes.word:\n",
    "            item = labels[labels.word == word].label.tolist()\n",
    "            item = [\"none\"] if item == [] else item\n",
    "            bbox_labels.extend(item)\n",
    "        bboxes[\"label\"] = bbox_labels\n",
    "        # now format as a dict to be fed into the processor\n",
    "        words = bboxes.word.tolist()\n",
    "        boxes = bboxes.bbox.tolist()\n",
    "        labels = bboxes.label.tolist()\n",
    "        encoding = self.processor(\n",
    "            image, \n",
    "            words, \n",
    "            boxes=boxes, \n",
    "            word_labels=labels, \n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        return encoding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1a7fa258",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = str(Path(\"ICDAR-2019-SROIE\").absolute())\n",
    "dataset = SROIEProcDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904bee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
